{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7af0431-fd54-4ae1-9a3e-a894868ff144",
   "metadata": {},
   "source": [
    "# Rubrix and Veganuary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3ced1-bc1e-4a46-a79d-3938a504af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install rubrix pandas datasets transformers[torch] gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc20a7d8-3249-45a4-8fb2-d7b5aae19e72",
   "metadata": {},
   "source": [
    "## Uploading the tweets for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2f7c48-283e-4658-bc4f-9d0fc5329994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tweets\n",
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_json('tweets.json')\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2d5c5-2d72-4061-b195-985b4e531b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we tokenize the tweets using spacy\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34da88-8291-4200-83fa-c610ed6b8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we preprocess the tweets with the pysentimiento library\n",
    "from pysentimiento.preprocessing import preprocess_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa825f5-c4e1-41ae-be32-44e7659373ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Rubrix records for annotations\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "records = []\n",
    "for index, row in tqdm(tweets.iterrows(), total=len(tweets)):  \n",
    "    text = preprocess_tweet(row[\"text\"], lang=\"en\")\n",
    "    \n",
    "    # spaCy Doc creation\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # get spacy tokens\n",
    "    tokens = [token.text for token in doc]\n",
    "    \n",
    "    # Rubrix TokenClassificationRecord list\n",
    "    records.append(\n",
    "        rb.TokenClassificationRecord(\n",
    "            text=text,\n",
    "            tokens=tokens,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e42577a-f79b-4bf0-a080-436672a9d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the records to Rubrix for annotating\n",
    "rb.log(records=records, name=\"veganuary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dbf9f3-9a89-4af3-8130-f5ac746b0666",
   "metadata": {},
   "source": [
    "## Load annotated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9869231-5038-4714-aa76-a596b4a0096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load only the annotated records from Rubrix (your/own NER/token classification task)\n",
    "tweets_df = rb.load('veganuary', query=\"status:Validated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb0af7-b60a-40f3-93d7-5763a6404ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad75621-70c2-4844-9d5b-df0c5f26268e",
   "metadata": {},
   "source": [
    "### Transform entities to bio tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e0346-8b48-4f30-a00b-9508a9c5220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform entity spans to bio tags\n",
    "from spacy.training import offsets_to_biluo_tags, biluo_to_iob\n",
    "tqdm.pandas()\n",
    "\n",
    "def entities_to_tags(row):\n",
    "    doc = nlp(row[\"text\"])\n",
    "    entities = [(entity[1], entity[2], entity[0]) for entity in row['annotation']]\n",
    "    biluo_tags = offsets_to_biluo_tags(doc, entities)\n",
    "    \n",
    "    if \"-\" in biluo_tags:\n",
    "        return None\n",
    "    \n",
    "    return biluo_to_iob(biluo_tags)\n",
    "\n",
    "tweets_df[\"ner_tags\"] = tweets_df.progress_apply(entities_to_tags, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58624df-834b-480c-9f01-999255f94582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove annotations that could not be transformed\n",
    "tweets_df = tweets_df.dropna(subset=[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f64df-9a4e-41e7-8118-3c294966db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a09d4-4fcb-454a-9962-180fc200014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tweets_df.ner_tags.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f913761a-be05-4f6a-862b-bb54b24c1e06",
   "metadata": {},
   "source": [
    "## Train a transformer\n",
    "\n",
    "Most of the stuff is a copy&paste from the transformers docs & examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7b844-75d1-4250-99ac-8ca0c51246af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, ClassLabel, Value\n",
    "\n",
    "# [ClassLabel] does not work with Dataset.from_pandas ...\n",
    "tweets_dict = {\"tokens\": list(tweets_df.tokens), \"ner_tags\": list(tweets_df.ner_tags)}\n",
    "\n",
    "label_list = [\"O\", \"B-food\", \"I-food\"]\n",
    "features = Features({\n",
    "    \"tokens\": [Value(\"string\")], \n",
    "    \"ner_tags\": [ClassLabel(names=label_list)]\n",
    "})\n",
    "\n",
    "tweets = Dataset.from_dict(tweets_dict, features=features)\n",
    "\n",
    "tweets = tweets.train_test_split(0.2, seed=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382238a-b876-473e-80de-d484cbc748ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base\", add_prefix_space=True, model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8c8f9-5489-41d2-9340-1148c6302502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975e6773-e1de-421f-83e5-386f7045bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweets = tweets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42cc721-f507-411f-b985-59bd855238d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb0727-1feb-43d3-aeea-6276e896b60e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109e50b-e0a1-4212-bc7d-45c25c3d7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=[\"wandb\"],\n",
    "    no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee3510-1702-4f86-91be-194d6ef77a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "# Metrics\n",
    "metric = load_metric(\"seqeval\")\n",
    "per_entity = False\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    if per_entity:\n",
    "        # Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4308fdfd-e5c1-4596-b862-e65161fe1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_tweets[\"train\"],\n",
    "    eval_dataset=tokenized_tweets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e69175-e69e-4355-bcd7-a710d0698b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d320d-e0ef-4eb2-ab29-40c79b741d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4690ff-b771-4372-b7fe-47b1282154e1",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096e04c-8271-4f36-9e03-95211b4cc9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ec988-cea4-470a-a571-a446df955d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e11e8-8bce-4c0d-a1c6-75df64f2ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Rubrix (your/own NER/token classification task)\n",
    "tweets_df = rb.load('veganuary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcfa512-f24a-43c6-9af0-7867d7cebf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f387ef-bcbc-4c9e-bec7-b2f2da65e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mentions(text):\n",
    "    # here we emulate the 'is_split_into_words', since the pipeline does not allow for tokenized input\n",
    "    doc = nlp(text)\n",
    "    text = \" \".join([token.text for token in doc])\n",
    "    \n",
    "    predictions = pl(text)\n",
    "    return [pred[\"word\"].strip() for pred in predictions if pred[\"entity_group\"] == \"LABEL_1\"]\n",
    "    \n",
    "tweets_df[\"mentions\"] = tweets_df.text.progress_map(extract_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25562b3f-113a-4418-9c6c-f091c905b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(tweets_df.mentions.sum()).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af95fd5d-f9f2-439c-844f-c6efa487f281",
   "metadata": {},
   "source": [
    "## push dataset and model to the HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d7197-1c98-4f48-ab90-269b880fdaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'unsupervised' split\n",
    "idx = tweets_df.status == \"Default\"\n",
    "\n",
    "tweets[\"unsupervised\"] = Dataset.from_dict(\n",
    "    {\"tokens\": tweets_df[idx].tokens}, \n",
    "    features=Features({\"tokens\": [Value(\"string\")]})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa3199-e06b-445a-88af-97dcff546728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to the HF Hub\n",
    "tweets.push_to_hub(\"Recognai/veganuary\", token=\"your-token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4278ef-06f1-4433-8e18-56b5e2fc696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push model to the HF Hub\n",
    "model.push_to_hub(\"veganuary_ner\", organization=\"Recognai\", use_auth_token=\"your-token\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
